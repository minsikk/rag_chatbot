# -*- coding: utf-8 -*-
"""rag-test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19MUWXoQNnAm_rvOx9oEsGb44dCuxfH1W
"""

from google.colab import drive
drive.mount("/gdrive", force_remount=True)

"""# ìƒ‰ì¸

## ë¬¸ì„œ ë¡œë“œ(Document Loading)
"""

!pip install langchain-community pypdf "unstructured[all-docs]"

import time
from langchain_community.document_loaders import UnstructuredFileLoader

# Colabì— ì—…ë¡œë“œí•œ PDF íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
file_path = "./insurance.pdf"  # ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()

try:
    # UnstructuredFileLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ PDF íŒŒì¼ ë¡œë“œ
    # UnstructuredLoaderëŠ” íŒŒì¼ ê²½ë¡œë§Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load()

    # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"PDF ë¡œë“œ ì™„ë£Œ. ì´ {len(docs)}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

    # ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë¯¸ë¦¬ë³´ê¸°
    if docs:
        print("\n--- ì²« ë²ˆì§¸ ë¬¸ì„œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ---")
        # UnstructuredLoaderëŠ” í˜ì´ì§€ ë‹¨ìœ„ê°€ ì•„ë‹Œ êµ¬ì¡°í™”ëœ ìš”ì†Œ(ì˜ˆ: í…ìŠ¤íŠ¸, ì œëª©, í…Œì´ë¸”)ë³„ë¡œ ë¬¸ì„œë¥¼ ë¶„í• í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë”°ë¼ì„œ docs ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ í˜ì´ì§€ ìˆ˜ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        print(docs[0].page_content[:500])  # ì²« 500ì ì¶œë ¥
        print(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {docs[0].metadata}")

except FileNotFoundError:
    print(f"ì˜¤ë¥˜: '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ Colabì— ì˜¬ë°”ë¥´ê²Œ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
except Exception as e:
    print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì „ì²´ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ í™•ì¸
print(docs)

# ê° ë¬¸ì„œ ê°ì²´ì˜ í˜ì´ì§€ ë‚´ìš©ê³¼ ë©”íƒ€ë°ì´í„° í™•ì¸
for i, doc in enumerate(docs):
    print(f"\n--- í˜ì´ì§€ {i+1} ---")
    print(f"í˜ì´ì§€ ë‚´ìš© (ì¼ë¶€): {doc.page_content[:200]}...")
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")

"""## ë¬¸ì„œ ë¶„í• """

import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader


start_time = time.time()

# RecursiveCharacterTextSplitter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # ë¶„í• í•  í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ ê¸¸ì´
    chunk_overlap=200,  # ë¶„í• ëœ í…ìŠ¤íŠ¸ ì¡°ê° ê°„ì˜ ì¤‘ë³µ ê¸¸ì´
    length_function=len, # ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜. ë³´í†µ lenì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    is_separator_regex=False, # êµ¬ë¶„ìê°€ ì •ê·œì‹ì¸ì§€ ì—¬ë¶€
)

# ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
chunks = text_splitter.split_documents(docs)

end_time = time.time()
elapsed_time = end_time - start_time

# ê²°ê³¼ í™•ì¸
print("\n--- ë¬¸ì„œ ë¶„í•  ê²°ê³¼ ---")
print(f"ë¶„í•  ì „ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}ê°œ")
print(f"ë¶„í•  í›„ ì²­í¬(Chunk) ê°œìˆ˜: {len(chunks)}ê°œ")
print(f"ì²« ë²ˆì§¸ ì²­í¬ì˜ ê¸¸ì´: {len(chunks[0].page_content)}ì")
print(f"ì‹¤í–‰ ì‹œê°„: {elapsed_time:.4f}ì´ˆ")

# ë¶„í• ëœ ì²­í¬ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
print("\n--- ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš© ---")
print(chunks[0].page_content)
print("\n--- ë‘ ë²ˆì§¸ ì²­í¬ ë‚´ìš© ---")
print(chunks[20].page_content)

!pip install sentence-transformers

import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings



# -----------------
# 2. ì„ë² ë”© ëª¨ë¸ ì‹¤í—˜ (ìˆ˜ì •ëœ ë¶€ë¶„)
# -----------------
print("\n--- ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ì‹¤í–‰ ---")
start_time = time.time()

# ì˜¬ë°”ë¥¸ ëª¨ë¸ëª…ì¸ 'BAAI/bge-m3'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
model_name = "BAAI/bge-m3"
model_kwargs = {"device": "cuda"}
encode_kwargs = {"normalize_embeddings": True}

embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# ëª¨ë“  ì²­í¬ë¥¼ ì„ë² ë”©
chunk_contents = [c.page_content for c in chunks]
chunk_embeddings = embeddings.embed_documents(chunk_contents)

end_time = time.time()
elapsed_time = end_time - start_time

print(f"ì„ë² ë”© ì™„ë£Œ. ì´ {len(chunk_embeddings)}ê°œì˜ ë²¡í„°ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"ì‹¤í–‰ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

# -----------------
# 3. ì„ë² ë”© ê²°ê³¼ í™•ì¸
# -----------------
print("\n--- ì„ë² ë”© ê²°ê³¼ í™•ì¸ ---")
is_count_match = len(chunks) == len(chunk_embeddings)
print(f"ì²­í¬ ê°œìˆ˜ì™€ ë²¡í„° ê°œìˆ˜ ì¼ì¹˜ ì—¬ë¶€: {is_count_match}")

if chunk_embeddings:
    vector_dimension = len(chunk_embeddings[0])
    print(f"ì²« ë²ˆì§¸ ë²¡í„°ì˜ ì°¨ì›(í¬ê¸°): {vector_dimension}")

!pip install faiss-cpu

from langchain_community.vectorstores import FAISS

# FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
# chunksëŠ” ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
# embeddingsëŠ” BGE ëª¨ë¸ì„ ë¡œë“œí•œ ê°ì²´ì…ë‹ˆë‹¤.
db = FAISS.from_documents(chunks, embeddings)

print("ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”© ì €ì¥ ì™„ë£Œ.")

# ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ì§ˆë¬¸ ì •ì˜
query = "ë³´í—˜ ë‚˜ì´ ê³„ì‚° ì˜ˆì‹œ"

# ìœ ì‚¬ì„± ê²€ìƒ‰ ì‹¤í–‰
# k=3ì€ ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ 3ê°œì˜ ë¬¸ì„œë¥¼ ì°¾ìœ¼ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
docs_faiss = db.similarity_search(query, k=3)

print("\n--- ìœ ì‚¬ì„± ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ ---")
for i, doc in enumerate(docs_faiss):
    print(f"\n[ë¬¸ì„œ {i+1}]")
    print(doc.page_content[:300] + "...") # ë¬¸ì„œ ë‚´ìš© ì¼ë¶€ë§Œ ì¶œë ¥
    print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")

"""# ê²€ìƒ‰ ë° ìƒì„±

## í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

!pip install -q langchain-huggingface

!pip install langchain-google-genai

import os
import time
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from google.colab import userdata

if 'db' in locals():
    print("FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # -----------------
    # LLM êµì²´ (ìˆ˜ì •ëœ ë¶€ë¶„)
    # -----------------
    # Google Gemini API í‚¤ ì„¤ì •
    # Colab > ë©”ë‰´(ğŸ”‘ë¹„ë°€ë³€ìˆ˜)ì— GOOGLE_API_KEYë¥¼ ë¨¼ì € ì„¤ì •í•˜ì„¸ìš”.
    google_api_key = userdata.get("GOOGLE_API_KEY")
    assert google_api_key, "Colab > ë©”ë‰´(ğŸ”‘ë¹„ë°€ë³€ìˆ˜)ì— GOOGLE_API_KEYë¥¼ ë¨¼ì € ì„¤ì •í•˜ì„¸ìš”."
    os.environ["GOOGLE_API_KEY"] = google_api_key

    # Gemini ëª¨ë¸ ë¡œë“œ
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.1)

    # ì§ì ‘ LLMì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì´ ì˜ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
    response = llm.invoke("ì•ˆë…•! ë„ˆëŠ” ì–´ë–¤ ëª¨ë¸ì´ì•¼?")
    print(response)

else:
    print("FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë‹¨ê³„ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ 'db' ë³€ìˆ˜ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")

# -----------------
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ë° RetrievalQA ì²´ì¸ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
# -----------------
template = """
ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ì— ë‹µí•˜ì„¸ìš”.
ì§ˆë¬¸ìëŠ” ë…¸ì¸ìœ¼ë¡œ ì–´ë ¤ìš´ ë§ì„ ì´í•´í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}
"""
prompt = PromptTemplate.from_template(template)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# -----------------
# ìµœì¢… ë‹µë³€ ìƒì„± (ì´ì „ê³¼ ë™ì¼)
# -----------------
print("\n--- RetrievalQA ì²´ì¸ ì‹¤í–‰ ---")
start_time = time.time()

question = "íœ´ëŒ€í’ˆ ì†í•´ ê´€ë ¨ ì¡°í•­ì„ ìš”ì•½í•´ì„œ ì•Œë ¤ì¤˜"

result = qa_chain.invoke(
    {"query": question}
)

end_time = time.time()
elapsed_time = end_time - start_time

print(f"\nìµœì¢… ë‹µë³€: {result['result']}")
print(f"ë‹µë³€ ìƒì„± ì‹œê°„: {elapsed_time:.2f}ì´ˆ")

print("\n--- ë‹µë³€ì— ì‚¬ìš©ëœ ì›ë³¸ ë¬¸ì„œ ---")
for doc in result['source_documents']:
    print(f"- ì¶œì²˜: {doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} (í˜ì´ì§€: {doc.metadata.get('page', 'ì•Œ ìˆ˜ ì—†ìŒ')})")
    print(f"  ë‚´ìš© (ì¼ë¶€): {doc.page_content[:200]}...")

