# -*- coding: utf-8 -*-
"""rag-test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19MUWXoQNnAm_rvOx9oEsGb44dCuxfH1W
"""

from google.colab import drive
drive.mount("/gdrive", force_remount=True)

"""# 색인

## 문서 로드(Document Loading)
"""

!pip install langchain-community pypdf "unstructured[all-docs]"

import time
from langchain_community.document_loaders import UnstructuredFileLoader

# Colab에 업로드한 PDF 파일의 경로를 지정합니다.
file_path = "./insurance.pdf"  # 실제 파일명으로 바꿔주세요.

# 시작 시간 기록
start_time = time.time()

try:
    # UnstructuredFileLoader를 사용하여 PDF 파일 로드
    # UnstructuredLoader는 파일 경로만 입력받습니다.
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load()

    # 종료 시간 기록 및 실행 시간 계산
    end_time = time.time()
    elapsed_time = end_time - start_time

    print(f"PDF 로드 완료. 총 {len(docs)}개의 문서가 로드되었습니다.")
    print(f"실행 시간: {elapsed_time:.2f}초")

    # 로드된 문서의 내용을 미리보기
    if docs:
        print("\n--- 첫 번째 문서 내용 미리보기 ---")
        # UnstructuredLoader는 페이지 단위가 아닌 구조화된 요소(예: 텍스트, 제목, 테이블)별로 문서를 분할할 수 있습니다.
        # 따라서 docs 리스트의 길이가 페이지 수와 다를 수 있습니다.
        print(docs[0].page_content[:500])  # 첫 500자 출력
        print(f"문서 메타데이터: {docs[0].metadata}")

except FileNotFoundError:
    print(f"오류: '{file_path}' 파일을 찾을 수 없습니다. 파일을 Colab에 올바르게 업로드했는지 확인해주세요.")
except Exception as e:
    print(f"오류가 발생했습니다: {e}")

# 전체 문서 객체 리스트 확인
print(docs)

# 각 문서 객체의 페이지 내용과 메타데이터 확인
for i, doc in enumerate(docs):
    print(f"\n--- 페이지 {i+1} ---")
    print(f"페이지 내용 (일부): {doc.page_content[:200]}...")
    print(f"메타데이터: {doc.metadata}")

"""## 문서 분할"""

import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader


start_time = time.time()

# RecursiveCharacterTextSplitter 인스턴스 생성
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # 분할할 텍스트의 최대 길이
    chunk_overlap=200,  # 분할된 텍스트 조각 간의 중복 길이
    length_function=len, # 길이를 계산하는 함수. 보통 len을 사용합니다.
    is_separator_regex=False, # 구분자가 정규식인지 여부
)

# 문서를 청크로 분할
chunks = text_splitter.split_documents(docs)

end_time = time.time()
elapsed_time = end_time - start_time

# 결과 확인
print("\n--- 문서 분할 결과 ---")
print(f"분할 전 문서 개수: {len(docs)}개")
print(f"분할 후 청크(Chunk) 개수: {len(chunks)}개")
print(f"첫 번째 청크의 길이: {len(chunks[0].page_content)}자")
print(f"실행 시간: {elapsed_time:.4f}초")

# 분할된 청크 내용 미리보기
print("\n--- 첫 번째 청크 내용 ---")
print(chunks[0].page_content)
print("\n--- 두 번째 청크 내용 ---")
print(chunks[20].page_content)

!pip install sentence-transformers

import time
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings



# -----------------
# 2. 임베딩 모델 실험 (수정된 부분)
# -----------------
print("\n--- 임베딩 모델 로드 및 실행 ---")
start_time = time.time()

# 올바른 모델명인 'BAAI/bge-m3'를 사용합니다.
model_name = "BAAI/bge-m3"
model_kwargs = {"device": "cuda"}
encode_kwargs = {"normalize_embeddings": True}

embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# 모든 청크를 임베딩
chunk_contents = [c.page_content for c in chunks]
chunk_embeddings = embeddings.embed_documents(chunk_contents)

end_time = time.time()
elapsed_time = end_time - start_time

print(f"임베딩 완료. 총 {len(chunk_embeddings)}개의 벡터가 생성되었습니다.")
print(f"실행 시간: {elapsed_time:.2f}초")

# -----------------
# 3. 임베딩 결과 확인
# -----------------
print("\n--- 임베딩 결과 확인 ---")
is_count_match = len(chunks) == len(chunk_embeddings)
print(f"청크 개수와 벡터 개수 일치 여부: {is_count_match}")

if chunk_embeddings:
    vector_dimension = len(chunk_embeddings[0])
    print(f"첫 번째 벡터의 차원(크기): {vector_dimension}")

!pip install faiss-cpu

from langchain_community.vectorstores import FAISS

# FAISS 벡터 데이터베이스 생성
# chunks는 이전 단계에서 생성된 문서 청크 리스트입니다.
# embeddings는 BGE 모델을 로드한 객체입니다.
db = FAISS.from_documents(chunks, embeddings)

print("벡터 데이터베이스에 임베딩 저장 완료.")

# 유사성 검색을 위한 질문 정의
query = "보험 나이 계산 예시"

# 유사성 검색 실행
# k=3은 가장 유사한 상위 3개의 문서를 찾으라는 의미입니다.
docs_faiss = db.similarity_search(query, k=3)

print("\n--- 유사성 검색 결과 확인 ---")
for i, doc in enumerate(docs_faiss):
    print(f"\n[문서 {i+1}]")
    print(doc.page_content[:300] + "...") # 문서 내용 일부만 출력
    print(f"메타데이터: {doc.metadata}")

"""# 검색 및 생성

## 프롬프트 생성
"""

!pip install -q langchain-huggingface

!pip install langchain-google-genai

import os
import time
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from google.colab import userdata

if 'db' in locals():
    print("FAISS 벡터 데이터베이스가 준비되었습니다.")

    # -----------------
    # LLM 교체 (수정된 부분)
    # -----------------
    # Google Gemini API 키 설정
    # Colab > 메뉴(🔑비밀변수)에 GOOGLE_API_KEY를 먼저 설정하세요.
    google_api_key = userdata.get("GOOGLE_API_KEY")
    assert google_api_key, "Colab > 메뉴(🔑비밀변수)에 GOOGLE_API_KEY를 먼저 설정하세요."
    os.environ["GOOGLE_API_KEY"] = google_api_key

    # Gemini 모델 로드
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.1)

    # 직접 LLM을 호출하여 모델이 잘 로드되었는지 확인
    response = llm.invoke("안녕! 너는 어떤 모델이야?")
    print(response)

else:
    print("FAISS 벡터 데이터베이스가 없습니다. 이전 단계를 다시 실행하여 'db' 변수를 생성해주세요.")

# -----------------
# 프롬프트 템플릿 정의 및 RetrievalQA 체인 생성 (이전과 동일)
# -----------------
template = """
다음 문맥(context)을 사용하여 질문(question)에 답하세요.
질문자는 노인으로 어려운 말을 이해하지 못하기 때문에 쉬운 말로 풀어서 설명하세요.
컨텍스트:
{context}

질문: {question}
"""
prompt = PromptTemplate.from_template(template)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff",
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt}
)

# -----------------
# 최종 답변 생성 (이전과 동일)
# -----------------
print("\n--- RetrievalQA 체인 실행 ---")
start_time = time.time()

question = "휴대품 손해 관련 조항을 요약해서 알려줘"

result = qa_chain.invoke(
    {"query": question}
)

end_time = time.time()
elapsed_time = end_time - start_time

print(f"\n최종 답변: {result['result']}")
print(f"답변 생성 시간: {elapsed_time:.2f}초")

print("\n--- 답변에 사용된 원본 문서 ---")
for doc in result['source_documents']:
    print(f"- 출처: {doc.metadata.get('source', '알 수 없음')} (페이지: {doc.metadata.get('page', '알 수 없음')})")
    print(f"  내용 (일부): {doc.page_content[:200]}...")

